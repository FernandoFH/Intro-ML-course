{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"(Solución) Exerc.10 - ","version":"0.3.2","provenance":[],"collapsed_sections":[]},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"metadata":{"id":"9AQmffVsO2Sn","colab_type":"text"},"cell_type":"markdown","source":["# Notebook 10 - Continuación con Redes Neuronales Multicapa.\n","\n","*   Recuerda que puedes consultar la documentación sobre una función escribiendo **?** justo después de la función: *Ejemplo: np.maximum?*\n","*   Puedes ejecutar el contenido de una celda con el atajo de teclado **CTRL+ENTER**\n","*   Utiliza **TAB** cada vez que quieras autocompletar una llamada a una función.\n","*   Puedes ejecutar instrucciones de bash directamente desde el notebook usando **!** : *Ejemplo: !pip install tensorflow*\n","*   Recuerda que Google es tu amigo, y saber buscar la información en las documentaciones de las librerías es muy importante.\n","*   Una solución correcta no es la que funciona sino la que se entiende!\n","*   No dudes en preguntar cualquier duda al profesor que lleva todo el día dando la turra."]},{"metadata":{"id":"xhnUB7b0PXWC","colab_type":"text"},"cell_type":"markdown","source":["## 1. ¿Qué tan robusto es tu modelo?\n","\n","Ayer comprobamos que haciendo uso de simple Red Neuronal Multicapa podemos obtener un buen rendimiento (aprox. 95%) de acierto en la predicción de números de MNIST. Pero... ¿Qué tan robusto es tu modelo? O dicho de otra manera, ¿cómo se ven afectadas las predicciones de tu modelo cuándo aparecen variaciones en tus datos de entrada?.\n","\n","Es decir, imagínate que las imágenes que le pasamos al modelo, se generan a partir de tomar una foto al dígito escrito manualmente con tu movil. Es probable que algunas de las fotos que queramos predecir no estén centradas (ej. a) o no correctamente alineadas (ej. b) o sufran de mala calidad (ej. c). ¿Crees que esto puede afectar al rendimiento de tu modelo?\n","\n","> ![texto alternativo](https://i.imgur.com/qs86wF1.jpg)\n","\n","---\n","\n","**Tu tarea:** Copia el código implementado ayer en Keras que entrena a tu red neuronal para predecir valores del dataset MNIST. Una vez lo tengas entrenado, realiza lo siguiente:\n","\n","1. (**bonus**) ¿Es necesario reentrenar siempre que quieras utilizar tu modelo? Investiga si hay alguna forma de guardar tus modelos entrenados, para así volver a cargarlos en un futuro cuando quieras utilizarlo. \n","2. Evalua los datos que tienes en tu ***test_set***. Después utiliza tu ***test_set*** para generar tres test de pruebas diferentes que incluyan las distorsiones que hemos mencionado antes (translaciones, rotaciones, y ruido 25% y ruido 50%). Por ejemplo, una translación la puedes conseguir cogiendo el vector de entrada de una de las imágenes, y eliminando columnas por la derecha/izquierda y arriba/abajo de la imagen, y rellenando con 0s las nuevas zonas de la imagen (o directamente usando las bondades de ***scipy.ndimage.shift()***). Una rotación es más complejo, pero por suerte scipy incluye funcionalidades que te pueden facilitar el trabajo (ej. ***scipy.ndimage.rotate()***). Finalmente, añadir ruido es simplemente, sumar a cada uno de los píxeles de tu imagen valores aleatorios (controla que en cualquier caso la intensidad de los píxeles se mantenga en el rango de valores esperados). Consigue que en cada dataset el grado de distorsión (ej. grado de inclinación) varíe por imagen. Visualiza algunas de estas imágenes distorsionadas.\n","\n","> **En traslación no cortes más del 25% de la imagen por cada eje. En rotación no rotes más de 90º. En ruido, añade una cantidad máxima de ruido de +-0.25 y +-0.5 (25% Ruido y 50% Ruido).**\n","\n","3. Antes de evaluar tus datasets distorsionas... Entendiendo el funcionamiento de la arquitectura de la Red Neuronal Multicapa ¿Cómo esperas que se va a comportar tu modelo?¿Soportará estas distorsiones?\n","\n","4. Evalua las predicciones para cada uno de los datasets y compara los resultados obtenidos. ¿Se te ocurre alguna forma de evitar esto?\n","\n","5. Utiliza la mismas funciones que has utilizado para distorsionar tus imágenes y genera imágenes distorsionadas de los elementos de tu ***training_set***. Genera con todas estas imágenes y las originales en tu ***training_set*** un nuevo ***training_set*** aumentado (a este proceso se le denomina ***Data Augmentation***). Entrena a tu modelo nuevamente con este training_set aumentado y obtén evaluaciones de nuevo de los ***test_set distorsionados*** que habíamos generado previamente. ¿Mejora la cosa?"]},{"metadata":{"id":"YaAJNYbLPLRU","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy as np\n","import scipy as sc\n","import sklearn as sk\n","import pandas  as pd\n","import seaborn as sb\n","import matplotlib.pyplot as plt\n","\n","# Cargamos el dataset desde el archivo.\n","mnist = pd.read_csv(\"./sample_data/mnist_train_small.csv\", header=None).as_matrix()\n","\n","# Guardamos las variables X e Y.\n","X, Y = mnist[:, 1:], mnist[:, 0:1]"],"execution_count":0,"outputs":[]},{"metadata":{"id":"VyvqSqKFcomZ","colab_type":"code","outputId":"7dca49ca-4fa6-48ac-c136-55b0ceb904a9","executionInfo":{"status":"ok","timestamp":1543621916687,"user_tz":0,"elapsed":2381,"user":{"displayName":"Carlos Santana","photoUrl":"https://lh4.googleusercontent.com/-CeMGa-V2idY/AAAAAAAAAAI/AAAAAAAAO2Q/22S5ucbTeu8/s64/photo.jpg","userId":"06972008324074016783"}},"colab":{"base_uri":"https://localhost:8080/","height":72}},"cell_type":"code","source":["import tensorflow as tf\n","\n","from tensorflow.keras.layers     import Dense\n","from tensorflow.keras.utils      import to_categorical\n","from tensorflow.keras.optimizers import SGD\n","\n","from sklearn.model_selection import train_test_split\n","\n","# Normalizamos input y codificamos output con one-hot encoding.\n","Xt = X / 255\n","Yt = to_categorical(Y, 10)\n","\n","# Generamos train y test set.\n","X_train, X_test, Y_train, Y_test = train_test_split(Xt, Yt, train_size=0.7)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["/usr/local/lib/python3.6/dist-packages/sklearn/model_selection/_split.py:2026: FutureWarning: From version 0.21, test_size will always complement train_size unless both are specified.\n","  FutureWarning)\n"],"name":"stderr"}]},{"metadata":{"id":"nyLr8wAGRvq0","colab_type":"code","outputId":"8003ca4d-464a-4385-95ff-4b34fe0a6617","executionInfo":{"status":"ok","timestamp":1543622053129,"user_tz":0,"elapsed":120709,"user":{"displayName":"Carlos Santana","photoUrl":"https://lh4.googleusercontent.com/-CeMGa-V2idY/AAAAAAAAAAI/AAAAAAAAO2Q/22S5ucbTeu8/s64/photo.jpg","userId":"06972008324074016783"}},"colab":{"base_uri":"https://localhost:8080/","height":3554}},"cell_type":"code","source":["# Inicializamos el modelo.\n","model = tf.keras.Sequential()\n","\n","# Adds a densely-connected layer with 64 units to the model:\n","model.add(Dense(128, activation='relu'))\n","# Add another:\n","model.add(Dense(64,  activation='relu'))\n","# Add another:\n","model.add(Dense(32,  activation='relu'))\n","# Add a softmax layer with 10 output units:\n","model.add(Dense(10, activation='softmax'))\n","\n","# Configure a model for mean-squared error regression.\n","model.compile(optimizer=SGD(lr=0.05),\n","              loss='categorical_crossentropy',   # mean squared error\n","              metrics=['acc'])              # mean absolute error\n","\n","\n","arr = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), epochs=100)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 14000 samples, validate on 6000 samples\n","Epoch 1/100\n","14000/14000 [==============================] - 2s 146us/step - loss: 0.7239 - acc: 0.7808 - val_loss: 0.3238 - val_acc: 0.9095\n","Epoch 2/100\n","14000/14000 [==============================] - 1s 90us/step - loss: 0.2730 - acc: 0.9190 - val_loss: 0.3426 - val_acc: 0.9002\n","Epoch 3/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 0.2016 - acc: 0.9389 - val_loss: 0.2081 - val_acc: 0.9418\n","Epoch 4/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 0.1590 - acc: 0.9529 - val_loss: 0.2662 - val_acc: 0.9230\n","Epoch 5/100\n","14000/14000 [==============================] - 1s 86us/step - loss: 0.1264 - acc: 0.9617 - val_loss: 0.1848 - val_acc: 0.9480\n","Epoch 6/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 0.1040 - acc: 0.9704 - val_loss: 0.1562 - val_acc: 0.9565\n","Epoch 7/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 0.0835 - acc: 0.9766 - val_loss: 0.1578 - val_acc: 0.9573\n","Epoch 8/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 0.0644 - acc: 0.9804 - val_loss: 0.1568 - val_acc: 0.9565\n","Epoch 9/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 0.0523 - acc: 0.9856 - val_loss: 0.2183 - val_acc: 0.9367\n","Epoch 10/100\n","14000/14000 [==============================] - 1s 86us/step - loss: 0.0436 - acc: 0.9881 - val_loss: 0.1541 - val_acc: 0.9602\n","Epoch 11/100\n","14000/14000 [==============================] - 1s 86us/step - loss: 0.0320 - acc: 0.9921 - val_loss: 0.1535 - val_acc: 0.9595\n","Epoch 12/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 0.0236 - acc: 0.9948 - val_loss: 0.1532 - val_acc: 0.9593\n","Epoch 13/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 0.0225 - acc: 0.9934 - val_loss: 0.1778 - val_acc: 0.9535\n","Epoch 14/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 0.0146 - acc: 0.9970 - val_loss: 0.1579 - val_acc: 0.9613\n","Epoch 15/100\n","14000/14000 [==============================] - 1s 86us/step - loss: 0.0089 - acc: 0.9991 - val_loss: 0.1606 - val_acc: 0.9615\n","Epoch 16/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 0.0073 - acc: 0.9993 - val_loss: 0.1683 - val_acc: 0.9607\n","Epoch 17/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 0.0052 - acc: 0.9997 - val_loss: 0.1701 - val_acc: 0.9602\n","Epoch 18/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 0.0043 - acc: 0.9999 - val_loss: 0.1696 - val_acc: 0.9613\n","Epoch 19/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 0.0036 - acc: 0.9999 - val_loss: 0.1706 - val_acc: 0.9625\n","Epoch 20/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 0.0029 - acc: 1.0000 - val_loss: 0.1745 - val_acc: 0.9632\n","Epoch 21/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 0.0025 - acc: 0.9999 - val_loss: 0.1750 - val_acc: 0.9612\n","Epoch 22/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 0.0023 - acc: 1.0000 - val_loss: 0.1768 - val_acc: 0.9625\n","Epoch 23/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 0.0019 - acc: 1.0000 - val_loss: 0.1806 - val_acc: 0.9612\n","Epoch 24/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 0.0017 - acc: 1.0000 - val_loss: 0.1817 - val_acc: 0.9622\n","Epoch 25/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 0.0016 - acc: 1.0000 - val_loss: 0.1821 - val_acc: 0.9625\n","Epoch 26/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 0.0015 - acc: 1.0000 - val_loss: 0.1856 - val_acc: 0.9620\n","Epoch 27/100\n","14000/14000 [==============================] - 1s 86us/step - loss: 0.0013 - acc: 1.0000 - val_loss: 0.1865 - val_acc: 0.9623\n","Epoch 28/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.1850 - val_acc: 0.9617\n","Epoch 29/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 0.0012 - acc: 1.0000 - val_loss: 0.1880 - val_acc: 0.9622\n","Epoch 30/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 0.0011 - acc: 1.0000 - val_loss: 0.1878 - val_acc: 0.9620\n","Epoch 31/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 0.0010 - acc: 1.0000 - val_loss: 0.1880 - val_acc: 0.9620\n","Epoch 32/100\n","14000/14000 [==============================] - 1s 86us/step - loss: 9.5857e-04 - acc: 1.0000 - val_loss: 0.1921 - val_acc: 0.9620\n","Epoch 33/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 9.0046e-04 - acc: 1.0000 - val_loss: 0.1932 - val_acc: 0.9620\n","Epoch 34/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 8.5637e-04 - acc: 1.0000 - val_loss: 0.1921 - val_acc: 0.9620\n","Epoch 35/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 8.1109e-04 - acc: 1.0000 - val_loss: 0.1938 - val_acc: 0.9617\n","Epoch 36/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 7.7077e-04 - acc: 1.0000 - val_loss: 0.1939 - val_acc: 0.9620\n","Epoch 37/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 7.3280e-04 - acc: 1.0000 - val_loss: 0.1979 - val_acc: 0.9620\n","Epoch 38/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 6.9752e-04 - acc: 1.0000 - val_loss: 0.1973 - val_acc: 0.9617\n","Epoch 39/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 6.7275e-04 - acc: 1.0000 - val_loss: 0.1982 - val_acc: 0.9617\n","Epoch 40/100\n","14000/14000 [==============================] - 1s 83us/step - loss: 6.4007e-04 - acc: 1.0000 - val_loss: 0.1986 - val_acc: 0.9618\n","Epoch 41/100\n","14000/14000 [==============================] - 1s 86us/step - loss: 6.1677e-04 - acc: 1.0000 - val_loss: 0.1984 - val_acc: 0.9610\n","Epoch 42/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 5.9532e-04 - acc: 1.0000 - val_loss: 0.2002 - val_acc: 0.9620\n","Epoch 43/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 5.6961e-04 - acc: 1.0000 - val_loss: 0.2021 - val_acc: 0.9620\n","Epoch 44/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 5.4864e-04 - acc: 1.0000 - val_loss: 0.2006 - val_acc: 0.9613\n","Epoch 45/100\n","14000/14000 [==============================] - 1s 86us/step - loss: 5.2973e-04 - acc: 1.0000 - val_loss: 0.2019 - val_acc: 0.9615\n","Epoch 46/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 5.0824e-04 - acc: 1.0000 - val_loss: 0.2022 - val_acc: 0.9613\n","Epoch 47/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 4.9327e-04 - acc: 1.0000 - val_loss: 0.2036 - val_acc: 0.9613\n","Epoch 48/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 4.7674e-04 - acc: 1.0000 - val_loss: 0.2035 - val_acc: 0.9612\n","Epoch 49/100\n","14000/14000 [==============================] - 1s 86us/step - loss: 4.6201e-04 - acc: 1.0000 - val_loss: 0.2055 - val_acc: 0.9613\n","Epoch 50/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 4.4805e-04 - acc: 1.0000 - val_loss: 0.2050 - val_acc: 0.9612\n","Epoch 51/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 4.3373e-04 - acc: 1.0000 - val_loss: 0.2049 - val_acc: 0.9615\n","Epoch 52/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 4.1922e-04 - acc: 1.0000 - val_loss: 0.2063 - val_acc: 0.9613\n","Epoch 53/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 4.0776e-04 - acc: 1.0000 - val_loss: 0.2073 - val_acc: 0.9613\n","Epoch 54/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 3.9548e-04 - acc: 1.0000 - val_loss: 0.2071 - val_acc: 0.9617\n","Epoch 55/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 3.8556e-04 - acc: 1.0000 - val_loss: 0.2082 - val_acc: 0.9612\n","Epoch 56/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 3.7505e-04 - acc: 1.0000 - val_loss: 0.2089 - val_acc: 0.9613\n","Epoch 57/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 3.6383e-04 - acc: 1.0000 - val_loss: 0.2089 - val_acc: 0.9612\n","Epoch 58/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 3.5393e-04 - acc: 1.0000 - val_loss: 0.2086 - val_acc: 0.9622\n","Epoch 59/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 3.4678e-04 - acc: 1.0000 - val_loss: 0.2097 - val_acc: 0.9612\n","Epoch 60/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 3.3794e-04 - acc: 1.0000 - val_loss: 0.2101 - val_acc: 0.9608\n","Epoch 61/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 3.3003e-04 - acc: 1.0000 - val_loss: 0.2109 - val_acc: 0.9613\n","Epoch 62/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 3.2187e-04 - acc: 1.0000 - val_loss: 0.2118 - val_acc: 0.9612\n","Epoch 63/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 3.1443e-04 - acc: 1.0000 - val_loss: 0.2119 - val_acc: 0.9612\n","Epoch 64/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 3.0710e-04 - acc: 1.0000 - val_loss: 0.2119 - val_acc: 0.9612\n","Epoch 65/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 2.9920e-04 - acc: 1.0000 - val_loss: 0.2120 - val_acc: 0.9612\n","Epoch 66/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 2.9344e-04 - acc: 1.0000 - val_loss: 0.2134 - val_acc: 0.9613\n","Epoch 67/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 2.8619e-04 - acc: 1.0000 - val_loss: 0.2139 - val_acc: 0.9612\n","Epoch 68/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 2.8069e-04 - acc: 1.0000 - val_loss: 0.2136 - val_acc: 0.9612\n","Epoch 69/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 2.7465e-04 - acc: 1.0000 - val_loss: 0.2150 - val_acc: 0.9610\n","Epoch 70/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 2.6856e-04 - acc: 1.0000 - val_loss: 0.2142 - val_acc: 0.9610\n","Epoch 71/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 2.6299e-04 - acc: 1.0000 - val_loss: 0.2150 - val_acc: 0.9610\n","Epoch 72/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 2.5794e-04 - acc: 1.0000 - val_loss: 0.2153 - val_acc: 0.9613\n","Epoch 73/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 2.5309e-04 - acc: 1.0000 - val_loss: 0.2160 - val_acc: 0.9610\n","Epoch 74/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 2.4766e-04 - acc: 1.0000 - val_loss: 0.2170 - val_acc: 0.9610\n","Epoch 75/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 2.4340e-04 - acc: 1.0000 - val_loss: 0.2163 - val_acc: 0.9610\n","Epoch 76/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 2.3828e-04 - acc: 1.0000 - val_loss: 0.2172 - val_acc: 0.9610\n","Epoch 77/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 2.3431e-04 - acc: 1.0000 - val_loss: 0.2167 - val_acc: 0.9612\n","Epoch 78/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 2.3028e-04 - acc: 1.0000 - val_loss: 0.2175 - val_acc: 0.9612\n","Epoch 79/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 2.2545e-04 - acc: 1.0000 - val_loss: 0.2183 - val_acc: 0.9610\n","Epoch 80/100\n","14000/14000 [==============================] - 1s 86us/step - loss: 2.2145e-04 - acc: 1.0000 - val_loss: 0.2179 - val_acc: 0.9615\n","Epoch 81/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 2.1816e-04 - acc: 1.0000 - val_loss: 0.2187 - val_acc: 0.9613\n","Epoch 82/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 2.1392e-04 - acc: 1.0000 - val_loss: 0.2193 - val_acc: 0.9613\n","Epoch 83/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 2.0998e-04 - acc: 1.0000 - val_loss: 0.2188 - val_acc: 0.9612\n","Epoch 84/100\n","14000/14000 [==============================] - 1s 83us/step - loss: 2.0731e-04 - acc: 1.0000 - val_loss: 0.2190 - val_acc: 0.9612\n","Epoch 85/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 2.0349e-04 - acc: 1.0000 - val_loss: 0.2196 - val_acc: 0.9610\n","Epoch 86/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 2.0059e-04 - acc: 1.0000 - val_loss: 0.2206 - val_acc: 0.9610\n","Epoch 87/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 1.9715e-04 - acc: 1.0000 - val_loss: 0.2209 - val_acc: 0.9610\n","Epoch 88/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 1.9384e-04 - acc: 1.0000 - val_loss: 0.2210 - val_acc: 0.9610\n","Epoch 89/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 1.9110e-04 - acc: 1.0000 - val_loss: 0.2210 - val_acc: 0.9612\n","Epoch 90/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 1.8743e-04 - acc: 1.0000 - val_loss: 0.2214 - val_acc: 0.9613\n","Epoch 91/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 1.8476e-04 - acc: 1.0000 - val_loss: 0.2218 - val_acc: 0.9615\n","Epoch 92/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 1.8206e-04 - acc: 1.0000 - val_loss: 0.2220 - val_acc: 0.9612\n","Epoch 93/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 1.7955e-04 - acc: 1.0000 - val_loss: 0.2220 - val_acc: 0.9612\n","Epoch 94/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 1.7726e-04 - acc: 1.0000 - val_loss: 0.2225 - val_acc: 0.9613\n","Epoch 95/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 1.7436e-04 - acc: 1.0000 - val_loss: 0.2228 - val_acc: 0.9610\n","Epoch 96/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 1.7130e-04 - acc: 1.0000 - val_loss: 0.2229 - val_acc: 0.9615\n","Epoch 97/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 1.6932e-04 - acc: 1.0000 - val_loss: 0.2237 - val_acc: 0.9612\n","Epoch 98/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 1.6621e-04 - acc: 1.0000 - val_loss: 0.2230 - val_acc: 0.9615\n","Epoch 99/100\n","14000/14000 [==============================] - 1s 85us/step - loss: 1.6489e-04 - acc: 1.0000 - val_loss: 0.2237 - val_acc: 0.9612\n","Epoch 100/100\n","14000/14000 [==============================] - 1s 84us/step - loss: 1.6253e-04 - acc: 1.0000 - val_loss: 0.2242 - val_acc: 0.9610\n"],"name":"stdout"}]},{"metadata":{"id":"tiEwAKIIYwBz","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy.random\n","\n","from scipy.ndimage import shift\n","from scipy.ndimage import rotate\n","\n","def traslate_imgs(X):\n","  \n","  # Matriz resultado.\n","  trasl_X = np.zeros(X.shape)\n","  \n","  for ix, x in enumerate(X):\n","    \n","    # Convertimos a matriz el vector de píxeles.\n","    rx = x.reshape(28, 28)\n","    #r Seleccionamos cuánto vamos a cortar en X e Y.\n","    shift_x = np.random.randint(14) - 7\n","    shift_y = np.random.randint(14) - 7\n","    # Guardamos la traslación de la imagen.\n","    trasl_X[ix] = shift(x.reshape(28, 28), (shift_x, shift_y)).flatten()\n","    \n","  return trasl_X\n","\n","\n","def rotate_imgs(X):\n","  \n","  # Matriz resultado.\n","  rot_X = np.zeros(X.shape)\n","  \n","  for ix, x in enumerate(X):\n","    \n","    # Convertimos a matriz el vector de píxeles.\n","    rx = x.reshape(28, 28)\n","    # Seleccionamos el ángulo con el que rotar la imagen.\n","    angle = np.random.randint(180) - 90\n","    # Guardamos la traslación de la imagen.\n","    rot_X[ix] = rotate(x.reshape(28, 28), angle, reshape=False).flatten()\n","    \n","  return rot_X\n","\n","\n","def noise_imgs(X, noise_level=0.5):\n","  \n","  # Matriz resultado.\n","  nois_X = np.zeros(X.shape)\n","  \n","  for ix, x in enumerate(X):\n","    \n","    # Convertimos a matriz el vector de píxeles.\n","    rx = x.reshape(28, 28)\n","    # Seleccionamos el ángulo con el que rotar la imagen.\n","    noise = (np.random.random(X[ix].shape) * 2 - 1) * noise_level\n","    # Guardamos la traslación de la imagen.\n","    nois_X[ix] = np.clip(x + noise, 0.0, 1.0).flatten()\n","    \n","  return nois_X"],"execution_count":0,"outputs":[]},{"metadata":{"id":"zf935Wj2PLVS","colab_type":"code","colab":{}},"cell_type":"code","source":["# Generamos validación trasladada.\n","trasX_test = traslate_imgs(X_test)\n","# Generamos validación rotada.\n","rotaX_test = rotate_imgs(X_test)\n","# Generamos validación con ruido 50%.\n","no50X_test = noise_imgs(X_test, 0.5)\n","# Generamos validación con ruido 25%.\n","no25X_test = noise_imgs(X_test, 0.25)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"umRaXIlvPfF5","colab_type":"code","outputId":"a35ef0ac-b1e6-4549-f368-c8ede1186f88","executionInfo":{"status":"ok","timestamp":1543578737392,"user_tz":0,"elapsed":1007,"user":{"displayName":"Carlos Santana","photoUrl":"https://lh4.googleusercontent.com/-CeMGa-V2idY/AAAAAAAAAAI/AAAAAAAAO2Q/22S5ucbTeu8/s64/photo.jpg","userId":"06972008324074016783"}},"colab":{"base_uri":"https://localhost:8080/","height":210}},"cell_type":"code","source":["idx = 0\n","\n","fig, axs = plt.subplots(1,4,figsize=(12,12))\n","\n","axs[0].matshow(trasX_test[idx,:].reshape(28, 28))\n","axs[1].matshow(rotaX_test[idx,:].reshape(28, 28))\n","axs[2].matshow(no50X_test[idx,:].reshape(28, 28))\n","axs[3].matshow(no25X_test[idx,:].reshape(28, 28))"],"execution_count":0,"outputs":[{"output_type":"execute_result","data":{"text/plain":["<matplotlib.image.AxesImage at 0x7f2301727e48>"]},"metadata":{"tags":[]},"execution_count":99},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAr4AAACwCAYAAADpEm0sAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<matplotlib.figure.Figure at 0x7f2301cc39e8>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"EjFPKZTGDP2a","colab_type":"code","outputId":"91296fcf-ee34-4657-bf97-fd4d688e3607","executionInfo":{"status":"ok","timestamp":1543578743134,"user_tz":0,"elapsed":1512,"user":{"displayName":"Carlos Santana","photoUrl":"https://lh4.googleusercontent.com/-CeMGa-V2idY/AAAAAAAAAAI/AAAAAAAAO2Q/22S5ucbTeu8/s64/photo.jpg","userId":"06972008324074016783"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"cell_type":"code","source":["print(\"Accuracy datos normales:\", model.evaluate(X_test,     Y_test, verbose=0)[1] * 100,\"%\")\n","print(\"Accuracy datos traslad.:\", model.evaluate(trasX_test, Y_test, verbose=0)[1] * 100,\"%\")\n","print(\"Accuracy datos rotation:\", model.evaluate(rotaX_test, Y_test, verbose=0)[1] * 100,\"%\")\n","print(\"Accuracy datos noise50%:\", model.evaluate(no50X_test, Y_test, verbose=0)[1] * 100,\"%\")\n","print(\"Accuracy datos noise25%:\", model.evaluate(no25X_test, Y_test, verbose=0)[1] * 100,\"%\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy datos normales: 99.06999982198079 %\n","Accuracy datos traslad.: 84.45333321889241 %\n","Accuracy datos rotation: 89.81499846776326 %\n","Accuracy datos noise50%: 93.96833216349285 %\n","Accuracy datos noise25%: 98.24833418528239 %\n"],"name":"stdout"}]},{"metadata":{"id":"DGTzEidcaZ5L","colab_type":"code","colab":{}},"cell_type":"code","source":["# Generamos train trasladada.\n","trasX_train = traslate_imgs(X_train)\n","# Generamos train rotada.\n","rotaX_train = rotate_imgs(X_train)\n","# Generamos train con ruido 50%.\n","no50X_train = noise_imgs(X_train, 0.5)\n","# Generamos train con ruido 25%.\n","no25X_train = noise_imgs(X_train, 0.25)\n","\n","# Juntamos todos los sets.\n","augmX_train = np.vstack([X_train, \n","                        trasX_train, \n","                        rotaX_train, \n","                        no50X_train, \n","                        no25X_train])\n","\n","# E incrementamos el vector Y por 5 veces.\n","augmY_train = np.tile(Y_train, (5,1))\n","\n","# Hacemos una selección aleatoria de los índices del vector.\n","idxs = np.random.choice(range(augmX_train.shape[0]), augmX_train.shape[0], replace=False)"],"execution_count":0,"outputs":[]},{"metadata":{"id":"yxEBUCr2eche","colab_type":"code","outputId":"98efe8bb-3018-40f4-a60e-d32d64e3862b","executionInfo":{"status":"ok","timestamp":1543579471369,"user_tz":0,"elapsed":1547,"user":{"displayName":"Carlos Santana","photoUrl":"https://lh4.googleusercontent.com/-CeMGa-V2idY/AAAAAAAAAAI/AAAAAAAAO2Q/22S5ucbTeu8/s64/photo.jpg","userId":"06972008324074016783"}},"colab":{"base_uri":"https://localhost:8080/","height":373}},"cell_type":"code","source":["augmX_train.shape\n","\n","i = 2\n","\n","plt.matshow(augmX_train[idxs][i,:].reshape(28,28))\n","print(augmY_train[idxs][i])"],"execution_count":0,"outputs":[{"output_type":"stream","text":["[0. 0. 0. 0. 1. 0. 0. 0. 0. 0.]\n"],"name":"stdout"},{"output_type":"display_data","data":{"image/png":"iVBORw0KGgoAAAANSUhEUgAAAVMAAAFSCAYAAABPFzzRAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n","text/plain":["<matplotlib.figure.Figure at 0x7f2300906f98>"]},"metadata":{"tags":[]}}]},{"metadata":{"id":"q9vooDAnbPSr","colab_type":"code","outputId":"f394f855-8847-4bfe-b327-dbc8f377f74a","executionInfo":{"status":"ok","timestamp":1543579792160,"user_tz":0,"elapsed":191425,"user":{"displayName":"Carlos Santana","photoUrl":"https://lh4.googleusercontent.com/-CeMGa-V2idY/AAAAAAAAAAI/AAAAAAAAO2Q/22S5ucbTeu8/s64/photo.jpg","userId":"06972008324074016783"}},"colab":{"base_uri":"https://localhost:8080/","height":897}},"cell_type":"code","source":["# Inicializamos el modelo.\n","model = tf.keras.Sequential()\n","\n","# Adds a densely-connected layer with 64 units to the model:\n","model.add(Dense(128, activation='relu'))\n","# Add another:\n","model.add(Dense(64,  activation='relu'))\n","# Add another:\n","model.add(Dense(32,  activation='relu'))\n","# Add a softmax layer with 10 output units:\n","model.add(Dense(10, activation='softmax'))\n","\n","# Configure a model for mean-squared error regression.\n","model.compile(optimizer=SGD(lr=0.05),\n","              loss='binary_crossentropy',   # mean squared error\n","              metrics=['acc'])              # mean absolute error\n","\n","arr = model.fit(augmX_train[idxs], augmY_train[idxs], epochs=25)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Epoch 1/25\n","70000/70000 [==============================] - 8s 120us/step - loss: 0.2314 - acc: 0.9226\n","Epoch 2/25\n","70000/70000 [==============================] - 7s 104us/step - loss: 0.1564 - acc: 0.9492\n","Epoch 3/25\n","70000/70000 [==============================] - 7s 104us/step - loss: 0.1311 - acc: 0.9564\n","Epoch 4/25\n","70000/70000 [==============================] - 8s 107us/step - loss: 0.1110 - acc: 0.9618\n","Epoch 5/25\n","70000/70000 [==============================] - 7s 106us/step - loss: 0.0951 - acc: 0.9665\n","Epoch 6/25\n","70000/70000 [==============================] - 8s 108us/step - loss: 0.0832 - acc: 0.9707\n","Epoch 7/25\n","70000/70000 [==============================] - 7s 107us/step - loss: 0.0739 - acc: 0.9739\n","Epoch 8/25\n","70000/70000 [==============================] - 8s 108us/step - loss: 0.0664 - acc: 0.9767\n","Epoch 9/25\n","70000/70000 [==============================] - 8s 109us/step - loss: 0.0601 - acc: 0.9789\n","Epoch 10/25\n","70000/70000 [==============================] - 8s 108us/step - loss: 0.0549 - acc: 0.9809\n","Epoch 11/25\n","70000/70000 [==============================] - 8s 110us/step - loss: 0.0503 - acc: 0.9827\n","Epoch 12/25\n","70000/70000 [==============================] - 8s 110us/step - loss: 0.0462 - acc: 0.9841\n","Epoch 13/25\n","70000/70000 [==============================] - 8s 111us/step - loss: 0.0430 - acc: 0.9852\n","Epoch 14/25\n","70000/70000 [==============================] - 8s 109us/step - loss: 0.0399 - acc: 0.9865\n","Epoch 15/25\n","70000/70000 [==============================] - 8s 109us/step - loss: 0.0372 - acc: 0.9875\n","Epoch 16/25\n","70000/70000 [==============================] - 9s 123us/step - loss: 0.0348 - acc: 0.9883\n","Epoch 17/25\n","70000/70000 [==============================] - 8s 108us/step - loss: 0.0325 - acc: 0.9891\n","Epoch 18/25\n","70000/70000 [==============================] - 8s 108us/step - loss: 0.0306 - acc: 0.9899\n","Epoch 19/25\n","70000/70000 [==============================] - 8s 108us/step - loss: 0.0287 - acc: 0.9907\n","Epoch 20/25\n","70000/70000 [==============================] - 7s 107us/step - loss: 0.0269 - acc: 0.9913\n","Epoch 21/25\n","70000/70000 [==============================] - 7s 105us/step - loss: 0.0254 - acc: 0.9918\n","Epoch 22/25\n","70000/70000 [==============================] - 7s 107us/step - loss: 0.0238 - acc: 0.9923\n","Epoch 23/25\n","70000/70000 [==============================] - 8s 108us/step - loss: 0.0224 - acc: 0.9929\n","Epoch 24/25\n","70000/70000 [==============================] - 7s 105us/step - loss: 0.0210 - acc: 0.9934\n","Epoch 25/25\n","70000/70000 [==============================] - 7s 106us/step - loss: 0.0199 - acc: 0.9939\n"],"name":"stdout"}]},{"metadata":{"id":"u7SzCBkQh4HM","colab_type":"code","outputId":"4518b3ab-912a-4174-add5-6077b898f075","executionInfo":{"status":"ok","timestamp":1543579905832,"user_tz":0,"elapsed":2455,"user":{"displayName":"Carlos Santana","photoUrl":"https://lh4.googleusercontent.com/-CeMGa-V2idY/AAAAAAAAAAI/AAAAAAAAO2Q/22S5ucbTeu8/s64/photo.jpg","userId":"06972008324074016783"}},"colab":{"base_uri":"https://localhost:8080/","height":105}},"cell_type":"code","source":["print(\"Accuracy datos normales:\", model.evaluate(X_test,     Y_test, verbose=0)[1] * 100,\"%\")\n","print(\"Accuracy datos traslad.:\", model.evaluate(trasX_test, Y_test, verbose=0)[1] * 100,\"%\")\n","print(\"Accuracy datos rotation:\", model.evaluate(rotaX_test, Y_test, verbose=0)[1] * 100,\"%\")\n","print(\"Accuracy datos noise50%:\", model.evaluate(no50X_test, Y_test, verbose=0)[1] * 100,\"%\")\n","print(\"Accuracy datos noise25%:\", model.evaluate(no25X_test, Y_test, verbose=0)[1] * 100,\"%\")"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Accuracy datos normales: 99.1049999554952 %\n","Accuracy datos traslad.: 96.05166664123536 %\n","Accuracy datos rotation: 97.50333344141642 %\n","Accuracy datos noise50%: 98.35666748682658 %\n","Accuracy datos noise25%: 99.01333352724711 %\n"],"name":"stdout"}]},{"metadata":{"id":"32oGijpim-51","colab_type":"text"},"cell_type":"markdown","source":["## 2. Analizando datos anteriores con Keras\n","\n","Ya que tenemos los conocimientos básicos para diseñar nuestras propias Redes Neuronales Multicapa con Keras, vamos a probar qué tan bien funciona las redes neuronales para los datos que analizamos la primera semana del Dataset de cancer de pecho (Exerc. 4 - Regresión Polinomial). Recordarás que probamos diferentes alternativas y obtuvimos en torno a un 95% de accuracy. ¿Obtendremos un mejor resultado con una Red Neuronal?\n","\n","\n","---\n","\n","**Tarea:** Diseña y entrena una Red Neuronal sobre el Wisconsin Breast Cancer dataset. Puedes reutilizar mucho del código desarollado en aquel ejercicio para el preprocesamiento de los datos. Sin embargo, suminístrale a la red neuronal todas las variables sin aplicar la reducción de dimensionalidad (PCA) vista posteriormente. Evalua el modelo y compáralo con las evaluaciones obtenidas en aquél ejercicio. Una vez finalizado, puedes probar a reducir la dimensionalidad de las variables con PCA y comparar los resultados.\n"]},{"metadata":{"id":"gCSpHr1fpfPt","colab_type":"code","colab":{}},"cell_type":"code","source":["import numpy   as np\n","import scipy   as sc\n","import pandas  as pd\n","import seaborn as sb\n","import matplotlib.pyplot as plt"],"execution_count":0,"outputs":[]},{"metadata":{"id":"wAC8nDyCpkHB","colab_type":"code","colab":{}},"cell_type":"code","source":["cancer_data = pd.read_csv(\"./breast_cancer_wisconsin.csv\", index_col=0)\n","\n","X = cancer_data.iloc[:, 1:]\n","Y = cancer_data.iloc[:, 0].map({'M':1, 'B':0})[:, np.newaxis]\n","\n","# Guardamos el nombre de las columnas.\n","cols = X.columns\n","\n","# COMIENZA AQUÍ TU CÓDIGO #\n","\n","tX = ((X - np.mean(X, axis=0)) / np.std(X, axis=0)).values\n","tY =  Y\n"],"execution_count":0,"outputs":[]},{"metadata":{"id":"Yw5YPAempwTJ","colab_type":"code","outputId":"05672170-5371-48e4-a063-9340f2852fca","executionInfo":{"status":"ok","timestamp":1543534248838,"user_tz":0,"elapsed":9611,"user":{"displayName":"Carlos Santana","photoUrl":"https://lh4.googleusercontent.com/-CeMGa-V2idY/AAAAAAAAAAI/AAAAAAAAO2Q/22S5ucbTeu8/s64/photo.jpg","userId":"06972008324074016783"}},"colab":{"base_uri":"https://localhost:8080/","height":7074}},"cell_type":"code","source":["# Generamos el train/test dataset.\n","X_train, X_test, Y_train, Y_test = train_test_split(tX, tY.ravel(), test_size=0.3)\n","\n","# Inicializamos el modelo.\n","model = tf.keras.Sequential()\n","\n","# Adds a densely-connected layer with 64 units to the model:\n","model.add(Dense(128, activation='relu'))\n","# Add another:\n","model.add(Dense(64,  activation='relu'))\n","# Add another:\n","model.add(Dense(32,  activation='relu'))\n","# Add a softmax layer with 10 output units:\n","model.add(Dense(1, activation='sigmoid'))\n","\n","# Configure a model for mean-squared error regression.\n","model.compile(optimizer=SGD(lr=0.001),\n","              loss='binary_crossentropy',   # mean squared error\n","              metrics=['acc'])              # mean absolute error\n","\n","arr = model.fit(X_train, Y_train, validation_data=(X_test, Y_test), batch_size=32, epochs=200)"],"execution_count":0,"outputs":[{"output_type":"stream","text":["Train on 398 samples, validate on 171 samples\n","Epoch 1/200\n","398/398 [==============================] - 2s 4ms/step - loss: 0.7277 - acc: 0.5905 - val_loss: 0.7027 - val_acc: 0.5673\n","Epoch 2/200\n","398/398 [==============================] - 0s 83us/step - loss: 0.7125 - acc: 0.5980 - val_loss: 0.6908 - val_acc: 0.5789\n","Epoch 3/200\n","398/398 [==============================] - 0s 75us/step - loss: 0.6978 - acc: 0.6030 - val_loss: 0.6791 - val_acc: 0.5965\n","Epoch 4/200\n","398/398 [==============================] - 0s 95us/step - loss: 0.6834 - acc: 0.6055 - val_loss: 0.6682 - val_acc: 0.6023\n","Epoch 5/200\n","398/398 [==============================] - 0s 78us/step - loss: 0.6703 - acc: 0.6080 - val_loss: 0.6576 - val_acc: 0.6082\n","Epoch 6/200\n","398/398 [==============================] - 0s 74us/step - loss: 0.6574 - acc: 0.6131 - val_loss: 0.6474 - val_acc: 0.6199\n","Epoch 7/200\n","398/398 [==============================] - 0s 77us/step - loss: 0.6452 - acc: 0.6156 - val_loss: 0.6377 - val_acc: 0.6199\n","Epoch 8/200\n","398/398 [==============================] - 0s 85us/step - loss: 0.6336 - acc: 0.6281 - val_loss: 0.6284 - val_acc: 0.6433\n","Epoch 9/200\n","398/398 [==============================] - 0s 80us/step - loss: 0.6222 - acc: 0.6457 - val_loss: 0.6195 - val_acc: 0.6491\n","Epoch 10/200\n","398/398 [==============================] - 0s 77us/step - loss: 0.6116 - acc: 0.6533 - val_loss: 0.6108 - val_acc: 0.6608\n","Epoch 11/200\n","398/398 [==============================] - 0s 80us/step - loss: 0.6015 - acc: 0.6734 - val_loss: 0.6025 - val_acc: 0.6784\n","Epoch 12/200\n","398/398 [==============================] - 0s 81us/step - loss: 0.5917 - acc: 0.6985 - val_loss: 0.5943 - val_acc: 0.6901\n","Epoch 13/200\n","398/398 [==============================] - 0s 75us/step - loss: 0.5821 - acc: 0.7337 - val_loss: 0.5865 - val_acc: 0.7076\n","Epoch 14/200\n","398/398 [==============================] - 0s 77us/step - loss: 0.5730 - acc: 0.7412 - val_loss: 0.5788 - val_acc: 0.7193\n","Epoch 15/200\n","398/398 [==============================] - 0s 81us/step - loss: 0.5642 - acc: 0.7764 - val_loss: 0.5714 - val_acc: 0.7368\n","Epoch 16/200\n","398/398 [==============================] - 0s 72us/step - loss: 0.5557 - acc: 0.7915 - val_loss: 0.5642 - val_acc: 0.7485\n","Epoch 17/200\n","398/398 [==============================] - 0s 73us/step - loss: 0.5475 - acc: 0.8141 - val_loss: 0.5570 - val_acc: 0.7602\n","Epoch 18/200\n","398/398 [==============================] - 0s 82us/step - loss: 0.5395 - acc: 0.8317 - val_loss: 0.5501 - val_acc: 0.7953\n","Epoch 19/200\n","398/398 [==============================] - 0s 77us/step - loss: 0.5317 - acc: 0.8492 - val_loss: 0.5432 - val_acc: 0.7953\n","Epoch 20/200\n","398/398 [==============================] - 0s 82us/step - loss: 0.5242 - acc: 0.8618 - val_loss: 0.5365 - val_acc: 0.8129\n","Epoch 21/200\n","398/398 [==============================] - 0s 89us/step - loss: 0.5168 - acc: 0.8618 - val_loss: 0.5298 - val_acc: 0.8129\n","Epoch 22/200\n","398/398 [==============================] - 0s 81us/step - loss: 0.5094 - acc: 0.8744 - val_loss: 0.5233 - val_acc: 0.8304\n","Epoch 23/200\n","398/398 [==============================] - 0s 86us/step - loss: 0.5023 - acc: 0.8819 - val_loss: 0.5168 - val_acc: 0.8363\n","Epoch 24/200\n","398/398 [==============================] - 0s 80us/step - loss: 0.4953 - acc: 0.8869 - val_loss: 0.5104 - val_acc: 0.8538\n","Epoch 25/200\n","398/398 [==============================] - 0s 78us/step - loss: 0.4886 - acc: 0.8920 - val_loss: 0.5042 - val_acc: 0.8655\n","Epoch 26/200\n","398/398 [==============================] - 0s 80us/step - loss: 0.4820 - acc: 0.8920 - val_loss: 0.4979 - val_acc: 0.8655\n","Epoch 27/200\n","398/398 [==============================] - 0s 74us/step - loss: 0.4755 - acc: 0.8920 - val_loss: 0.4919 - val_acc: 0.8655\n","Epoch 28/200\n","398/398 [==============================] - 0s 82us/step - loss: 0.4692 - acc: 0.8970 - val_loss: 0.4859 - val_acc: 0.8655\n","Epoch 29/200\n","398/398 [==============================] - 0s 79us/step - loss: 0.4630 - acc: 0.8995 - val_loss: 0.4801 - val_acc: 0.8713\n","Epoch 30/200\n","398/398 [==============================] - 0s 85us/step - loss: 0.4570 - acc: 0.8995 - val_loss: 0.4743 - val_acc: 0.8713\n","Epoch 31/200\n","398/398 [==============================] - 0s 76us/step - loss: 0.4509 - acc: 0.8995 - val_loss: 0.4687 - val_acc: 0.8772\n","Epoch 32/200\n","398/398 [==============================] - 0s 72us/step - loss: 0.4451 - acc: 0.9020 - val_loss: 0.4630 - val_acc: 0.8830\n","Epoch 33/200\n","398/398 [==============================] - 0s 84us/step - loss: 0.4393 - acc: 0.9020 - val_loss: 0.4576 - val_acc: 0.8830\n","Epoch 34/200\n","398/398 [==============================] - 0s 92us/step - loss: 0.4337 - acc: 0.9020 - val_loss: 0.4522 - val_acc: 0.8830\n","Epoch 35/200\n","398/398 [==============================] - 0s 75us/step - loss: 0.4282 - acc: 0.9020 - val_loss: 0.4469 - val_acc: 0.8830\n","Epoch 36/200\n","398/398 [==============================] - 0s 74us/step - loss: 0.4228 - acc: 0.9045 - val_loss: 0.4417 - val_acc: 0.8947\n","Epoch 37/200\n","398/398 [==============================] - 0s 83us/step - loss: 0.4175 - acc: 0.9070 - val_loss: 0.4365 - val_acc: 0.9006\n","Epoch 38/200\n","398/398 [==============================] - 0s 82us/step - loss: 0.4121 - acc: 0.9070 - val_loss: 0.4314 - val_acc: 0.9006\n","Epoch 39/200\n","398/398 [==============================] - 0s 75us/step - loss: 0.4070 - acc: 0.9121 - val_loss: 0.4265 - val_acc: 0.9064\n","Epoch 40/200\n","398/398 [==============================] - 0s 77us/step - loss: 0.4019 - acc: 0.9121 - val_loss: 0.4215 - val_acc: 0.9123\n","Epoch 41/200\n","398/398 [==============================] - 0s 72us/step - loss: 0.3969 - acc: 0.9095 - val_loss: 0.4166 - val_acc: 0.9123\n","Epoch 42/200\n","398/398 [==============================] - 0s 83us/step - loss: 0.3920 - acc: 0.9121 - val_loss: 0.4118 - val_acc: 0.9123\n","Epoch 43/200\n","398/398 [==============================] - 0s 75us/step - loss: 0.3873 - acc: 0.9146 - val_loss: 0.4071 - val_acc: 0.9181\n","Epoch 44/200\n","398/398 [==============================] - 0s 73us/step - loss: 0.3825 - acc: 0.9171 - val_loss: 0.4024 - val_acc: 0.9181\n","Epoch 45/200\n","398/398 [==============================] - 0s 74us/step - loss: 0.3779 - acc: 0.9171 - val_loss: 0.3978 - val_acc: 0.9181\n","Epoch 46/200\n","398/398 [==============================] - 0s 75us/step - loss: 0.3734 - acc: 0.9171 - val_loss: 0.3933 - val_acc: 0.9181\n","Epoch 47/200\n","398/398 [==============================] - 0s 99us/step - loss: 0.3689 - acc: 0.9171 - val_loss: 0.3888 - val_acc: 0.9298\n","Epoch 48/200\n","398/398 [==============================] - 0s 80us/step - loss: 0.3645 - acc: 0.9171 - val_loss: 0.3844 - val_acc: 0.9298\n","Epoch 49/200\n","398/398 [==============================] - 0s 78us/step - loss: 0.3602 - acc: 0.9196 - val_loss: 0.3801 - val_acc: 0.9298\n","Epoch 50/200\n","398/398 [==============================] - 0s 78us/step - loss: 0.3561 - acc: 0.9196 - val_loss: 0.3759 - val_acc: 0.9298\n","Epoch 51/200\n","398/398 [==============================] - 0s 83us/step - loss: 0.3520 - acc: 0.9196 - val_loss: 0.3717 - val_acc: 0.9298\n","Epoch 52/200\n","398/398 [==============================] - 0s 75us/step - loss: 0.3480 - acc: 0.9196 - val_loss: 0.3676 - val_acc: 0.9298\n","Epoch 53/200\n","398/398 [==============================] - 0s 85us/step - loss: 0.3441 - acc: 0.9196 - val_loss: 0.3635 - val_acc: 0.9298\n","Epoch 54/200\n","398/398 [==============================] - 0s 88us/step - loss: 0.3402 - acc: 0.9221 - val_loss: 0.3596 - val_acc: 0.9298\n","Epoch 55/200\n","398/398 [==============================] - 0s 77us/step - loss: 0.3365 - acc: 0.9271 - val_loss: 0.3556 - val_acc: 0.9298\n","Epoch 56/200\n","398/398 [==============================] - 0s 77us/step - loss: 0.3328 - acc: 0.9271 - val_loss: 0.3518 - val_acc: 0.9298\n","Epoch 57/200\n","398/398 [==============================] - 0s 82us/step - loss: 0.3292 - acc: 0.9271 - val_loss: 0.3480 - val_acc: 0.9298\n","Epoch 58/200\n","398/398 [==============================] - 0s 77us/step - loss: 0.3257 - acc: 0.9271 - val_loss: 0.3443 - val_acc: 0.9298\n","Epoch 59/200\n","398/398 [==============================] - 0s 90us/step - loss: 0.3222 - acc: 0.9296 - val_loss: 0.3406 - val_acc: 0.9298\n","Epoch 60/200\n","398/398 [==============================] - 0s 74us/step - loss: 0.3188 - acc: 0.9296 - val_loss: 0.3370 - val_acc: 0.9357\n","Epoch 61/200\n","398/398 [==============================] - 0s 72us/step - loss: 0.3156 - acc: 0.9296 - val_loss: 0.3335 - val_acc: 0.9357\n","Epoch 62/200\n","398/398 [==============================] - 0s 77us/step - loss: 0.3123 - acc: 0.9296 - val_loss: 0.3300 - val_acc: 0.9357\n","Epoch 63/200\n","398/398 [==============================] - 0s 78us/step - loss: 0.3092 - acc: 0.9296 - val_loss: 0.3267 - val_acc: 0.9474\n","Epoch 64/200\n","398/398 [==============================] - 0s 86us/step - loss: 0.3061 - acc: 0.9271 - val_loss: 0.3234 - val_acc: 0.9474\n","Epoch 65/200\n","398/398 [==============================] - 0s 83us/step - loss: 0.3031 - acc: 0.9271 - val_loss: 0.3201 - val_acc: 0.9474\n","Epoch 66/200\n","398/398 [==============================] - 0s 78us/step - loss: 0.3002 - acc: 0.9296 - val_loss: 0.3169 - val_acc: 0.9474\n","Epoch 67/200\n","398/398 [==============================] - 0s 84us/step - loss: 0.2973 - acc: 0.9296 - val_loss: 0.3138 - val_acc: 0.9474\n","Epoch 68/200\n","398/398 [==============================] - 0s 74us/step - loss: 0.2945 - acc: 0.9296 - val_loss: 0.3107 - val_acc: 0.9474\n","Epoch 69/200\n","398/398 [==============================] - 0s 70us/step - loss: 0.2917 - acc: 0.9296 - val_loss: 0.3076 - val_acc: 0.9474\n","Epoch 70/200\n","398/398 [==============================] - 0s 76us/step - loss: 0.2890 - acc: 0.9296 - val_loss: 0.3047 - val_acc: 0.9474\n","Epoch 71/200\n","398/398 [==============================] - 0s 81us/step - loss: 0.2864 - acc: 0.9296 - val_loss: 0.3017 - val_acc: 0.9474\n","Epoch 72/200\n","398/398 [==============================] - 0s 94us/step - loss: 0.2838 - acc: 0.9296 - val_loss: 0.2988 - val_acc: 0.9474\n","Epoch 73/200\n","398/398 [==============================] - 0s 82us/step - loss: 0.2812 - acc: 0.9296 - val_loss: 0.2959 - val_acc: 0.9532\n","Epoch 74/200\n","398/398 [==============================] - 0s 83us/step - loss: 0.2787 - acc: 0.9271 - val_loss: 0.2931 - val_acc: 0.9532\n","Epoch 75/200\n","398/398 [==============================] - 0s 81us/step - loss: 0.2763 - acc: 0.9271 - val_loss: 0.2903 - val_acc: 0.9532\n","Epoch 76/200\n","398/398 [==============================] - 0s 83us/step - loss: 0.2739 - acc: 0.9271 - val_loss: 0.2876 - val_acc: 0.9532\n","Epoch 77/200\n","398/398 [==============================] - 0s 80us/step - loss: 0.2715 - acc: 0.9271 - val_loss: 0.2849 - val_acc: 0.9532\n","Epoch 78/200\n","398/398 [==============================] - 0s 77us/step - loss: 0.2691 - acc: 0.9271 - val_loss: 0.2822 - val_acc: 0.9532\n","Epoch 79/200\n","398/398 [==============================] - 0s 81us/step - loss: 0.2668 - acc: 0.9271 - val_loss: 0.2797 - val_acc: 0.9532\n","Epoch 80/200\n","398/398 [==============================] - 0s 77us/step - loss: 0.2646 - acc: 0.9271 - val_loss: 0.2771 - val_acc: 0.9532\n","Epoch 81/200\n","398/398 [==============================] - 0s 96us/step - loss: 0.2624 - acc: 0.9271 - val_loss: 0.2746 - val_acc: 0.9532\n","Epoch 82/200\n","398/398 [==============================] - 0s 84us/step - loss: 0.2603 - acc: 0.9271 - val_loss: 0.2721 - val_acc: 0.9532\n","Epoch 83/200\n","398/398 [==============================] - 0s 80us/step - loss: 0.2581 - acc: 0.9271 - val_loss: 0.2697 - val_acc: 0.9532\n","Epoch 84/200\n","398/398 [==============================] - 0s 76us/step - loss: 0.2561 - acc: 0.9271 - val_loss: 0.2673 - val_acc: 0.9532\n","Epoch 85/200\n","398/398 [==============================] - 0s 79us/step - loss: 0.2540 - acc: 0.9271 - val_loss: 0.2649 - val_acc: 0.9532\n","Epoch 86/200\n","398/398 [==============================] - 0s 82us/step - loss: 0.2520 - acc: 0.9271 - val_loss: 0.2626 - val_acc: 0.9532\n","Epoch 87/200\n","398/398 [==============================] - 0s 80us/step - loss: 0.2500 - acc: 0.9271 - val_loss: 0.2603 - val_acc: 0.9532\n","Epoch 88/200\n","398/398 [==============================] - 0s 86us/step - loss: 0.2480 - acc: 0.9271 - val_loss: 0.2581 - val_acc: 0.9532\n","Epoch 89/200\n","398/398 [==============================] - 0s 76us/step - loss: 0.2461 - acc: 0.9271 - val_loss: 0.2558 - val_acc: 0.9532\n","Epoch 90/200\n","398/398 [==============================] - 0s 85us/step - loss: 0.2442 - acc: 0.9271 - val_loss: 0.2537 - val_acc: 0.9532\n","Epoch 91/200\n","398/398 [==============================] - 0s 84us/step - loss: 0.2424 - acc: 0.9271 - val_loss: 0.2515 - val_acc: 0.9532\n","Epoch 92/200\n","398/398 [==============================] - 0s 76us/step - loss: 0.2406 - acc: 0.9271 - val_loss: 0.2493 - val_acc: 0.9532\n","Epoch 93/200\n","398/398 [==============================] - 0s 79us/step - loss: 0.2388 - acc: 0.9271 - val_loss: 0.2472 - val_acc: 0.9532\n","Epoch 94/200\n","398/398 [==============================] - 0s 73us/step - loss: 0.2370 - acc: 0.9271 - val_loss: 0.2452 - val_acc: 0.9532\n","Epoch 95/200\n","398/398 [==============================] - 0s 82us/step - loss: 0.2353 - acc: 0.9271 - val_loss: 0.2431 - val_acc: 0.9532\n","Epoch 96/200\n","398/398 [==============================] - 0s 82us/step - loss: 0.2336 - acc: 0.9271 - val_loss: 0.2411 - val_acc: 0.9591\n","Epoch 97/200\n","398/398 [==============================] - 0s 78us/step - loss: 0.2319 - acc: 0.9271 - val_loss: 0.2391 - val_acc: 0.9591\n","Epoch 98/200\n","398/398 [==============================] - 0s 81us/step - loss: 0.2303 - acc: 0.9271 - val_loss: 0.2372 - val_acc: 0.9591\n","Epoch 99/200\n","398/398 [==============================] - 0s 84us/step - loss: 0.2286 - acc: 0.9271 - val_loss: 0.2353 - val_acc: 0.9591\n","Epoch 100/200\n","398/398 [==============================] - 0s 83us/step - loss: 0.2270 - acc: 0.9271 - val_loss: 0.2334 - val_acc: 0.9591\n","Epoch 101/200\n","398/398 [==============================] - 0s 83us/step - loss: 0.2255 - acc: 0.9271 - val_loss: 0.2316 - val_acc: 0.9591\n","Epoch 102/200\n","398/398 [==============================] - 0s 79us/step - loss: 0.2239 - acc: 0.9271 - val_loss: 0.2298 - val_acc: 0.9591\n","Epoch 103/200\n","398/398 [==============================] - 0s 82us/step - loss: 0.2224 - acc: 0.9271 - val_loss: 0.2280 - val_acc: 0.9591\n","Epoch 104/200\n","398/398 [==============================] - 0s 81us/step - loss: 0.2209 - acc: 0.9271 - val_loss: 0.2262 - val_acc: 0.9591\n","Epoch 105/200\n","398/398 [==============================] - 0s 81us/step - loss: 0.2195 - acc: 0.9296 - val_loss: 0.2245 - val_acc: 0.9591\n","Epoch 106/200\n","398/398 [==============================] - 0s 83us/step - loss: 0.2180 - acc: 0.9347 - val_loss: 0.2227 - val_acc: 0.9591\n","Epoch 107/200\n","398/398 [==============================] - 0s 78us/step - loss: 0.2166 - acc: 0.9347 - val_loss: 0.2211 - val_acc: 0.9591\n","Epoch 108/200\n","398/398 [==============================] - 0s 82us/step - loss: 0.2152 - acc: 0.9347 - val_loss: 0.2194 - val_acc: 0.9591\n","Epoch 109/200\n","398/398 [==============================] - 0s 76us/step - loss: 0.2138 - acc: 0.9347 - val_loss: 0.2179 - val_acc: 0.9591\n","Epoch 110/200\n","398/398 [==============================] - 0s 83us/step - loss: 0.2125 - acc: 0.9397 - val_loss: 0.2162 - val_acc: 0.9591\n","Epoch 111/200\n","398/398 [==============================] - 0s 80us/step - loss: 0.2111 - acc: 0.9397 - val_loss: 0.2147 - val_acc: 0.9591\n","Epoch 112/200\n","398/398 [==============================] - 0s 81us/step - loss: 0.2098 - acc: 0.9397 - val_loss: 0.2131 - val_acc: 0.9591\n","Epoch 113/200\n","398/398 [==============================] - 0s 78us/step - loss: 0.2085 - acc: 0.9397 - val_loss: 0.2116 - val_acc: 0.9591\n","Epoch 114/200\n","398/398 [==============================] - 0s 85us/step - loss: 0.2072 - acc: 0.9397 - val_loss: 0.2100 - val_acc: 0.9591\n","Epoch 115/200\n","398/398 [==============================] - 0s 83us/step - loss: 0.2059 - acc: 0.9397 - val_loss: 0.2085 - val_acc: 0.9591\n","Epoch 116/200\n","398/398 [==============================] - 0s 72us/step - loss: 0.2047 - acc: 0.9397 - val_loss: 0.2070 - val_acc: 0.9591\n","Epoch 117/200\n","398/398 [==============================] - 0s 81us/step - loss: 0.2035 - acc: 0.9397 - val_loss: 0.2056 - val_acc: 0.9591\n","Epoch 118/200\n","398/398 [==============================] - 0s 75us/step - loss: 0.2023 - acc: 0.9397 - val_loss: 0.2042 - val_acc: 0.9591\n","Epoch 119/200\n","398/398 [==============================] - 0s 70us/step - loss: 0.2011 - acc: 0.9397 - val_loss: 0.2028 - val_acc: 0.9591\n","Epoch 120/200\n","398/398 [==============================] - 0s 87us/step - loss: 0.1999 - acc: 0.9397 - val_loss: 0.2013 - val_acc: 0.9591\n","Epoch 121/200\n","398/398 [==============================] - 0s 83us/step - loss: 0.1987 - acc: 0.9397 - val_loss: 0.2000 - val_acc: 0.9591\n","Epoch 122/200\n","398/398 [==============================] - 0s 97us/step - loss: 0.1975 - acc: 0.9397 - val_loss: 0.1986 - val_acc: 0.9591\n","Epoch 123/200\n","398/398 [==============================] - 0s 82us/step - loss: 0.1964 - acc: 0.9372 - val_loss: 0.1972 - val_acc: 0.9591\n","Epoch 124/200\n","398/398 [==============================] - 0s 74us/step - loss: 0.1953 - acc: 0.9397 - val_loss: 0.1959 - val_acc: 0.9591\n","Epoch 125/200\n","398/398 [==============================] - 0s 76us/step - loss: 0.1941 - acc: 0.9372 - val_loss: 0.1946 - val_acc: 0.9708\n","Epoch 126/200\n","398/398 [==============================] - 0s 78us/step - loss: 0.1930 - acc: 0.9372 - val_loss: 0.1933 - val_acc: 0.9708\n","Epoch 127/200\n","398/398 [==============================] - 0s 88us/step - loss: 0.1920 - acc: 0.9372 - val_loss: 0.1921 - val_acc: 0.9708\n","Epoch 128/200\n","398/398 [==============================] - 0s 81us/step - loss: 0.1909 - acc: 0.9372 - val_loss: 0.1908 - val_acc: 0.9708\n","Epoch 129/200\n","398/398 [==============================] - 0s 88us/step - loss: 0.1898 - acc: 0.9372 - val_loss: 0.1895 - val_acc: 0.9708\n","Epoch 130/200\n","398/398 [==============================] - 0s 92us/step - loss: 0.1888 - acc: 0.9372 - val_loss: 0.1883 - val_acc: 0.9708\n","Epoch 131/200\n","398/398 [==============================] - 0s 76us/step - loss: 0.1877 - acc: 0.9397 - val_loss: 0.1871 - val_acc: 0.9708\n","Epoch 132/200\n","398/398 [==============================] - 0s 82us/step - loss: 0.1867 - acc: 0.9397 - val_loss: 0.1859 - val_acc: 0.9708\n","Epoch 133/200\n","398/398 [==============================] - 0s 101us/step - loss: 0.1857 - acc: 0.9397 - val_loss: 0.1847 - val_acc: 0.9708\n","Epoch 134/200\n","398/398 [==============================] - 0s 82us/step - loss: 0.1847 - acc: 0.9397 - val_loss: 0.1836 - val_acc: 0.9708\n","Epoch 135/200\n","398/398 [==============================] - 0s 79us/step - loss: 0.1837 - acc: 0.9397 - val_loss: 0.1824 - val_acc: 0.9708\n","Epoch 136/200\n","398/398 [==============================] - 0s 75us/step - loss: 0.1827 - acc: 0.9397 - val_loss: 0.1813 - val_acc: 0.9708\n","Epoch 137/200\n","398/398 [==============================] - 0s 86us/step - loss: 0.1817 - acc: 0.9397 - val_loss: 0.1802 - val_acc: 0.9708\n","Epoch 138/200\n","398/398 [==============================] - 0s 78us/step - loss: 0.1808 - acc: 0.9422 - val_loss: 0.1790 - val_acc: 0.9708\n","Epoch 139/200\n","398/398 [==============================] - 0s 80us/step - loss: 0.1798 - acc: 0.9422 - val_loss: 0.1779 - val_acc: 0.9708\n","Epoch 140/200\n","398/398 [==============================] - 0s 90us/step - loss: 0.1788 - acc: 0.9422 - val_loss: 0.1768 - val_acc: 0.9708\n","Epoch 141/200\n","398/398 [==============================] - 0s 80us/step - loss: 0.1779 - acc: 0.9422 - val_loss: 0.1757 - val_acc: 0.9708\n","Epoch 142/200\n","398/398 [==============================] - 0s 80us/step - loss: 0.1769 - acc: 0.9447 - val_loss: 0.1746 - val_acc: 0.9708\n","Epoch 143/200\n","398/398 [==============================] - 0s 77us/step - loss: 0.1760 - acc: 0.9447 - val_loss: 0.1736 - val_acc: 0.9708\n","Epoch 144/200\n","398/398 [==============================] - 0s 77us/step - loss: 0.1751 - acc: 0.9447 - val_loss: 0.1725 - val_acc: 0.9708\n","Epoch 145/200\n","398/398 [==============================] - 0s 77us/step - loss: 0.1742 - acc: 0.9447 - val_loss: 0.1715 - val_acc: 0.9708\n","Epoch 146/200\n","398/398 [==============================] - 0s 88us/step - loss: 0.1733 - acc: 0.9447 - val_loss: 0.1705 - val_acc: 0.9708\n","Epoch 147/200\n","398/398 [==============================] - 0s 87us/step - loss: 0.1724 - acc: 0.9447 - val_loss: 0.1695 - val_acc: 0.9708\n","Epoch 148/200\n","398/398 [==============================] - 0s 74us/step - loss: 0.1715 - acc: 0.9447 - val_loss: 0.1685 - val_acc: 0.9708\n","Epoch 149/200\n","398/398 [==============================] - 0s 82us/step - loss: 0.1707 - acc: 0.9447 - val_loss: 0.1675 - val_acc: 0.9708\n","Epoch 150/200\n","398/398 [==============================] - 0s 83us/step - loss: 0.1698 - acc: 0.9447 - val_loss: 0.1665 - val_acc: 0.9708\n","Epoch 151/200\n","398/398 [==============================] - 0s 76us/step - loss: 0.1690 - acc: 0.9447 - val_loss: 0.1656 - val_acc: 0.9708\n","Epoch 152/200\n","398/398 [==============================] - 0s 81us/step - loss: 0.1681 - acc: 0.9447 - val_loss: 0.1646 - val_acc: 0.9708\n","Epoch 153/200\n","398/398 [==============================] - 0s 81us/step - loss: 0.1673 - acc: 0.9447 - val_loss: 0.1637 - val_acc: 0.9708\n","Epoch 154/200\n","398/398 [==============================] - 0s 86us/step - loss: 0.1664 - acc: 0.9447 - val_loss: 0.1628 - val_acc: 0.9708\n","Epoch 155/200\n","398/398 [==============================] - 0s 84us/step - loss: 0.1656 - acc: 0.9447 - val_loss: 0.1619 - val_acc: 0.9708\n","Epoch 156/200\n","398/398 [==============================] - 0s 75us/step - loss: 0.1648 - acc: 0.9447 - val_loss: 0.1610 - val_acc: 0.9708\n","Epoch 157/200\n","398/398 [==============================] - 0s 80us/step - loss: 0.1640 - acc: 0.9447 - val_loss: 0.1601 - val_acc: 0.9708\n","Epoch 158/200\n","398/398 [==============================] - 0s 78us/step - loss: 0.1632 - acc: 0.9447 - val_loss: 0.1592 - val_acc: 0.9708\n","Epoch 159/200\n","398/398 [==============================] - 0s 83us/step - loss: 0.1624 - acc: 0.9447 - val_loss: 0.1583 - val_acc: 0.9708\n","Epoch 160/200\n","398/398 [==============================] - 0s 82us/step - loss: 0.1616 - acc: 0.9447 - val_loss: 0.1575 - val_acc: 0.9708\n","Epoch 161/200\n","398/398 [==============================] - 0s 82us/step - loss: 0.1608 - acc: 0.9447 - val_loss: 0.1566 - val_acc: 0.9708\n","Epoch 162/200\n","398/398 [==============================] - 0s 84us/step - loss: 0.1601 - acc: 0.9447 - val_loss: 0.1558 - val_acc: 0.9708\n","Epoch 163/200\n","398/398 [==============================] - 0s 84us/step - loss: 0.1593 - acc: 0.9447 - val_loss: 0.1549 - val_acc: 0.9708\n","Epoch 164/200\n","398/398 [==============================] - 0s 84us/step - loss: 0.1585 - acc: 0.9447 - val_loss: 0.1541 - val_acc: 0.9708\n","Epoch 165/200\n","398/398 [==============================] - 0s 76us/step - loss: 0.1578 - acc: 0.9447 - val_loss: 0.1532 - val_acc: 0.9708\n","Epoch 166/200\n","398/398 [==============================] - 0s 88us/step - loss: 0.1570 - acc: 0.9447 - val_loss: 0.1524 - val_acc: 0.9708\n","Epoch 167/200\n","398/398 [==============================] - 0s 82us/step - loss: 0.1563 - acc: 0.9472 - val_loss: 0.1516 - val_acc: 0.9708\n","Epoch 168/200\n","398/398 [==============================] - 0s 85us/step - loss: 0.1556 - acc: 0.9472 - val_loss: 0.1508 - val_acc: 0.9708\n","Epoch 169/200\n","398/398 [==============================] - 0s 79us/step - loss: 0.1548 - acc: 0.9472 - val_loss: 0.1501 - val_acc: 0.9708\n","Epoch 170/200\n","398/398 [==============================] - 0s 78us/step - loss: 0.1541 - acc: 0.9472 - val_loss: 0.1493 - val_acc: 0.9708\n","Epoch 171/200\n","398/398 [==============================] - 0s 74us/step - loss: 0.1534 - acc: 0.9472 - val_loss: 0.1485 - val_acc: 0.9708\n","Epoch 172/200\n","398/398 [==============================] - 0s 87us/step - loss: 0.1527 - acc: 0.9472 - val_loss: 0.1478 - val_acc: 0.9708\n","Epoch 173/200\n","398/398 [==============================] - 0s 80us/step - loss: 0.1520 - acc: 0.9472 - val_loss: 0.1470 - val_acc: 0.9708\n","Epoch 174/200\n","398/398 [==============================] - 0s 85us/step - loss: 0.1514 - acc: 0.9472 - val_loss: 0.1463 - val_acc: 0.9708\n","Epoch 175/200\n","398/398 [==============================] - 0s 83us/step - loss: 0.1507 - acc: 0.9472 - val_loss: 0.1456 - val_acc: 0.9708\n","Epoch 176/200\n","398/398 [==============================] - 0s 76us/step - loss: 0.1500 - acc: 0.9497 - val_loss: 0.1449 - val_acc: 0.9708\n","Epoch 177/200\n","398/398 [==============================] - 0s 74us/step - loss: 0.1494 - acc: 0.9497 - val_loss: 0.1442 - val_acc: 0.9708\n","Epoch 178/200\n","398/398 [==============================] - 0s 82us/step - loss: 0.1487 - acc: 0.9497 - val_loss: 0.1435 - val_acc: 0.9766\n","Epoch 179/200\n","398/398 [==============================] - 0s 85us/step - loss: 0.1481 - acc: 0.9497 - val_loss: 0.1428 - val_acc: 0.9766\n","Epoch 180/200\n","398/398 [==============================] - 0s 80us/step - loss: 0.1474 - acc: 0.9497 - val_loss: 0.1421 - val_acc: 0.9766\n","Epoch 181/200\n","398/398 [==============================] - 0s 76us/step - loss: 0.1468 - acc: 0.9497 - val_loss: 0.1414 - val_acc: 0.9766\n","Epoch 182/200\n","398/398 [==============================] - 0s 83us/step - loss: 0.1462 - acc: 0.9497 - val_loss: 0.1407 - val_acc: 0.9766\n","Epoch 183/200\n","398/398 [==============================] - 0s 73us/step - loss: 0.1456 - acc: 0.9497 - val_loss: 0.1401 - val_acc: 0.9766\n","Epoch 184/200\n","398/398 [==============================] - 0s 84us/step - loss: 0.1449 - acc: 0.9497 - val_loss: 0.1394 - val_acc: 0.9766\n","Epoch 185/200\n","398/398 [==============================] - 0s 94us/step - loss: 0.1443 - acc: 0.9497 - val_loss: 0.1388 - val_acc: 0.9766\n","Epoch 186/200\n","398/398 [==============================] - 0s 76us/step - loss: 0.1437 - acc: 0.9497 - val_loss: 0.1381 - val_acc: 0.9766\n","Epoch 187/200\n","398/398 [==============================] - 0s 73us/step - loss: 0.1431 - acc: 0.9497 - val_loss: 0.1375 - val_acc: 0.9766\n","Epoch 188/200\n","398/398 [==============================] - 0s 79us/step - loss: 0.1426 - acc: 0.9497 - val_loss: 0.1369 - val_acc: 0.9766\n","Epoch 189/200\n","398/398 [==============================] - 0s 76us/step - loss: 0.1419 - acc: 0.9497 - val_loss: 0.1363 - val_acc: 0.9766\n","Epoch 190/200\n","398/398 [==============================] - 0s 83us/step - loss: 0.1413 - acc: 0.9497 - val_loss: 0.1356 - val_acc: 0.9766\n","Epoch 191/200\n","398/398 [==============================] - 0s 74us/step - loss: 0.1407 - acc: 0.9497 - val_loss: 0.1350 - val_acc: 0.9766\n","Epoch 192/200\n","398/398 [==============================] - 0s 82us/step - loss: 0.1402 - acc: 0.9497 - val_loss: 0.1344 - val_acc: 0.9766\n","Epoch 193/200\n","398/398 [==============================] - 0s 94us/step - loss: 0.1396 - acc: 0.9497 - val_loss: 0.1338 - val_acc: 0.9766\n","Epoch 194/200\n","398/398 [==============================] - 0s 79us/step - loss: 0.1390 - acc: 0.9523 - val_loss: 0.1332 - val_acc: 0.9766\n","Epoch 195/200\n","398/398 [==============================] - 0s 80us/step - loss: 0.1385 - acc: 0.9523 - val_loss: 0.1327 - val_acc: 0.9766\n","Epoch 196/200\n","398/398 [==============================] - 0s 85us/step - loss: 0.1379 - acc: 0.9523 - val_loss: 0.1321 - val_acc: 0.9766\n","Epoch 197/200\n","398/398 [==============================] - 0s 84us/step - loss: 0.1373 - acc: 0.9523 - val_loss: 0.1315 - val_acc: 0.9766\n","Epoch 198/200\n","398/398 [==============================] - 0s 87us/step - loss: 0.1368 - acc: 0.9523 - val_loss: 0.1310 - val_acc: 0.9766\n","Epoch 199/200\n","398/398 [==============================] - 0s 76us/step - loss: 0.1363 - acc: 0.9548 - val_loss: 0.1304 - val_acc: 0.9766\n","Epoch 200/200\n","398/398 [==============================] - 0s 73us/step - loss: 0.1357 - acc: 0.9548 - val_loss: 0.1299 - val_acc: 0.9766\n"],"name":"stdout"}]},{"metadata":{"id":"dbfaETwUCsTR","colab_type":"code","outputId":"a70d2ed2-d350-477e-9b7f-c39a0313e123","executionInfo":{"status":"ok","timestamp":1543584896307,"user_tz":0,"elapsed":905,"user":{"displayName":"Carlos Santana","photoUrl":"https://lh4.googleusercontent.com/-CeMGa-V2idY/AAAAAAAAAAI/AAAAAAAAO2Q/22S5ucbTeu8/s64/photo.jpg","userId":"06972008324074016783"}},"colab":{"base_uri":"https://localhost:8080/","height":281}},"cell_type":"code","source":["model.summary()\n"],"execution_count":0,"outputs":[{"output_type":"stream","text":["_________________________________________________________________\n","Layer (type)                 Output Shape              Param #   \n","=================================================================\n","dense_44 (Dense)             multiple                  100480    \n","_________________________________________________________________\n","dense_45 (Dense)             multiple                  8256      \n","_________________________________________________________________\n","dense_46 (Dense)             multiple                  2080      \n","_________________________________________________________________\n","dense_47 (Dense)             multiple                  330       \n","=================================================================\n","Total params: 111,146\n","Trainable params: 111,146\n","Non-trainable params: 0\n","_________________________________________________________________\n"],"name":"stdout"}]}]}